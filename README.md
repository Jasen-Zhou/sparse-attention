# sparse-attention
自适应的稀疏注意力，主要方法是在第一个Transformer层近似一个低秩的注意力分数矩阵，以此为依据构造掩码矩阵

#原始的Attention效果（163M的smallGPT2）
还在跑
